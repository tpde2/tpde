; NOTE: Assertions have been autogenerated by test/update_tpde_llc_test_checks.py UTC_ARGS: --version 5
; SPDX-FileCopyrightText: 2025 Contributors to TPDE <https://tpde.org>
; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

; RUN: tpde-llc --target=x86_64 %s | %objdump | FileCheck %s -check-prefixes=X64
; RUN: tpde-llc --target=aarch64 %s | %objdump | FileCheck %s -check-prefixes=ARM64

define i8 @vr_mul_v1i8(ptr %p) {
; X64-LABEL: <vr_mul_v1i8>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    movzx eax, byte ptr [rdi]
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    mov eax, ecx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v1i8>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldrb w1, [x0]
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <1 x i8>, ptr %p
  %r = call i8 @llvm.vector.reduce.mul(<1 x i8> %v)
  ret i8 %r
}

define i8 @vr_mul_v5i8(ptr %p) {
; X64-LABEL: <vr_mul_v5i8>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    push rbx
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    movzx eax, byte ptr [rdi]
; X64-NEXT:    movzx ecx, byte ptr [rdi + 0x1]
; X64-NEXT:    movzx edx, byte ptr [rdi + 0x2]
; X64-NEXT:    movzx ebx, byte ptr [rdi + 0x3]
; X64-NEXT:    movzx esi, byte ptr [rdi + 0x4]
; X64-NEXT:    mov edi, eax
; X64-NEXT:    mov r8d, ecx
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov r8d, edx
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov r8d, ebx
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov r8d, esi
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov eax, edi
; X64-NEXT:    pop rbx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v5i8>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldrb w1, [x0]
; ARM64-NEXT:    ldrb w2, [x0, #0x1]
; ARM64-NEXT:    ldrb w3, [x0, #0x2]
; ARM64-NEXT:    ldrb w4, [x0, #0x3]
; ARM64-NEXT:    ldrb w5, [x0, #0x4]
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    mov w6, w2
; ARM64-NEXT:    mul w6, w6, w0
; ARM64-NEXT:    mov w0, w3
; ARM64-NEXT:    mul w0, w0, w6
; ARM64-NEXT:    mov w6, w4
; ARM64-NEXT:    mul w6, w6, w0
; ARM64-NEXT:    mov w0, w5
; ARM64-NEXT:    mul w0, w0, w6
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <5 x i8>, ptr %p
  %r = call i8 @llvm.vector.reduce.mul(<5 x i8> %v)
  ret i8 %r
}

define i8 @vr_mul_v8i8(ptr %p) {
; X64-LABEL: <vr_mul_v8i8>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    movsd xmm0, qword ptr [rdi]
; X64-NEXT:    movq qword ptr [rbp - 0x30], xmm0
; X64-NEXT:    movzx eax, byte ptr [rbp - 0x30]
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x2f]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x2e]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x2d]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x2c]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x2b]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x2a]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x29]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v8i8>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr d0, [x0]
; ARM64-NEXT:    umov w0, v0.b[0]
; ARM64-NEXT:    umov w1, v0.b[1]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[2]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[3]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[4]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[5]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[6]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[7]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <8 x i8>, ptr %p
  %r = call i8 @llvm.vector.reduce.mul(<8 x i8> %v)
  ret i8 %r
}

define i8 @vr_mul_v16i8(ptr %p) {
; X64-LABEL: <vr_mul_v16i8>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x40], xmm0
; X64-NEXT:    movzx eax, byte ptr [rbp - 0x40]
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3f]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3e]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3d]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3c]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3b]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3a]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x39]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x38]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x37]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x36]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x35]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x34]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x33]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x32]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x31]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v16i8>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    umov w0, v0.b[0]
; ARM64-NEXT:    umov w1, v0.b[1]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[2]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[3]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[4]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[5]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[6]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[7]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[8]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[9]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[10]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[11]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[12]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[13]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[14]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[15]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <16 x i8>, ptr %p
  %r = call i8 @llvm.vector.reduce.mul(<16 x i8> %v)
  ret i8 %r
}

define i8 @vr_mul_v32i8(ptr %p) {
; X64-LABEL: <vr_mul_v32i8>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x50], xmm0
; X64-NEXT:    movzx eax, byte ptr [rbp - 0x50]
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x4f]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x4e]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x4d]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x4c]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x4b]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x4a]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x49]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x48]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x47]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x46]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x45]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x44]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x43]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x42]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x41]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movapd xmmword ptr [rbp - 0x40], xmm1
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x40]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3f]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3e]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3d]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3c]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3b]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x3a]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x39]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x38]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x37]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x36]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x35]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x34]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x33]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x32]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, byte ptr [rbp - 0x31]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v32i8>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    umov w0, v0.b[0]
; ARM64-NEXT:    umov w1, v0.b[1]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[2]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[3]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[4]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[5]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[6]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[7]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[8]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[9]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[10]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[11]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[12]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[13]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.b[14]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.b[15]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.b[0]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.b[1]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.b[2]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.b[3]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.b[4]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.b[5]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.b[6]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.b[7]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.b[8]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.b[9]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.b[10]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.b[11]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.b[12]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.b[13]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.b[14]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.b[15]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <32 x i8>, ptr %p
  %r = call i8 @llvm.vector.reduce.mul(<32 x i8> %v)
  ret i8 %r
}

define i16 @vr_mul_v1i16(ptr %p) {
; X64-LABEL: <vr_mul_v1i16>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    movzx eax, word ptr [rdi]
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    mov eax, ecx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v1i16>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldrh w1, [x0]
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <1 x i16>, ptr %p
  %r = call i16 @llvm.vector.reduce.mul(<1 x i16> %v)
  ret i16 %r
}

define i16 @vr_mul_v5i16(ptr %p) {
; X64-LABEL: <vr_mul_v5i16>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    push rbx
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    movzx eax, word ptr [rdi]
; X64-NEXT:    movzx ecx, word ptr [rdi + 0x2]
; X64-NEXT:    movzx edx, word ptr [rdi + 0x4]
; X64-NEXT:    movzx ebx, word ptr [rdi + 0x6]
; X64-NEXT:    movzx esi, word ptr [rdi + 0x8]
; X64-NEXT:    mov edi, eax
; X64-NEXT:    mov r8d, ecx
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov r8d, edx
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov r8d, ebx
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov r8d, esi
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov eax, edi
; X64-NEXT:    pop rbx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v5i16>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldrh w1, [x0]
; ARM64-NEXT:    ldrh w2, [x0, #0x2]
; ARM64-NEXT:    ldrh w3, [x0, #0x4]
; ARM64-NEXT:    ldrh w4, [x0, #0x6]
; ARM64-NEXT:    ldrh w5, [x0, #0x8]
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    mov w6, w2
; ARM64-NEXT:    mul w6, w6, w0
; ARM64-NEXT:    mov w0, w3
; ARM64-NEXT:    mul w0, w0, w6
; ARM64-NEXT:    mov w6, w4
; ARM64-NEXT:    mul w6, w6, w0
; ARM64-NEXT:    mov w0, w5
; ARM64-NEXT:    mul w0, w0, w6
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <5 x i16>, ptr %p
  %r = call i16 @llvm.vector.reduce.mul(<5 x i16> %v)
  ret i16 %r
}

define i16 @vr_mul_v8i16(ptr %p) {
; X64-LABEL: <vr_mul_v8i16>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x40], xmm0
; X64-NEXT:    movzx eax, word ptr [rbp - 0x40]
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x3e]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x3c]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x3a]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x38]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x36]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x34]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x32]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v8i16>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    umov w0, v0.h[0]
; ARM64-NEXT:    umov w1, v0.h[1]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.h[2]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.h[3]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.h[4]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.h[5]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.h[6]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.h[7]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <8 x i16>, ptr %p
  %r = call i16 @llvm.vector.reduce.mul(<8 x i16> %v)
  ret i16 %r
}

define i16 @vr_mul_v16i16(ptr %p) {
; X64-LABEL: <vr_mul_v16i16>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x50], xmm0
; X64-NEXT:    movzx eax, word ptr [rbp - 0x50]
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x4e]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x4c]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x4a]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x48]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x46]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x44]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x42]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movapd xmmword ptr [rbp - 0x40], xmm1
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x40]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x3e]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x3c]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x3a]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x38]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x36]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x34]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movzx ecx, word ptr [rbp - 0x32]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v16i16>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    umov w0, v0.h[0]
; ARM64-NEXT:    umov w1, v0.h[1]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.h[2]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.h[3]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.h[4]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.h[5]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v0.h[6]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v0.h[7]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.h[0]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.h[1]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.h[2]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.h[3]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.h[4]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.h[5]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    umov w0, v1.h[6]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    umov w1, v1.h[7]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <16 x i16>, ptr %p
  %r = call i16 @llvm.vector.reduce.mul(<16 x i16> %v)
  ret i16 %r
}

define i32 @vr_mul_v1i32(ptr %p) {
; X64-LABEL: <vr_mul_v1i32>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    mov eax, dword ptr [rdi]
; X64-NEXT:    mov ecx, eax
; X64-NEXT:    mov eax, ecx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v1i32>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr w1, [x0]
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <1 x i32>, ptr %p
  %r = call i32 @llvm.vector.reduce.mul(<1 x i32> %v)
  ret i32 %r
}

define i32 @vr_mul_v4i32(ptr %p) {
; X64-LABEL: <vr_mul_v4i32>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x40], xmm0
; X64-NEXT:    mov eax, dword ptr [rbp - 0x40]
; X64-NEXT:    mov ecx, dword ptr [rbp - 0x3c]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    mov ecx, dword ptr [rbp - 0x38]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    mov ecx, dword ptr [rbp - 0x34]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v4i32>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    mov w0, v0.s[0]
; ARM64-NEXT:    mov w1, v0.s[1]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, v0.s[2]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    mov w1, v0.s[3]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <4 x i32>, ptr %p
  %r = call i32 @llvm.vector.reduce.mul(<4 x i32> %v)
  ret i32 %r
}

define i32 @vr_mul_v5i32(ptr %p) {
; X64-LABEL: <vr_mul_v5i32>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    push rbx
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    mov eax, dword ptr [rdi]
; X64-NEXT:    mov ecx, dword ptr [rdi + 0x4]
; X64-NEXT:    mov edx, dword ptr [rdi + 0x8]
; X64-NEXT:    mov ebx, dword ptr [rdi + 0xc]
; X64-NEXT:    mov esi, dword ptr [rdi + 0x10]
; X64-NEXT:    mov edi, eax
; X64-NEXT:    mov r8d, ecx
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov r8d, edx
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov r8d, ebx
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov r8d, esi
; X64-NEXT:    imul edi, r8d
; X64-NEXT:    mov eax, edi
; X64-NEXT:    pop rbx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v5i32>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr w1, [x0]
; ARM64-NEXT:    ldr w2, [x0, #0x4]
; ARM64-NEXT:    ldr w3, [x0, #0x8]
; ARM64-NEXT:    ldr w4, [x0, #0xc]
; ARM64-NEXT:    ldr w5, [x0, #0x10]
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    mov w6, w2
; ARM64-NEXT:    mul w6, w6, w0
; ARM64-NEXT:    mov w0, w3
; ARM64-NEXT:    mul w0, w0, w6
; ARM64-NEXT:    mov w6, w4
; ARM64-NEXT:    mul w6, w6, w0
; ARM64-NEXT:    mov w0, w5
; ARM64-NEXT:    mul w0, w0, w6
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <5 x i32>, ptr %p
  %r = call i32 @llvm.vector.reduce.mul(<5 x i32> %v)
  ret i32 %r
}

define i32 @vr_mul_v8i32(ptr %p) {
; X64-LABEL: <vr_mul_v8i32>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x50], xmm0
; X64-NEXT:    mov eax, dword ptr [rbp - 0x50]
; X64-NEXT:    mov ecx, dword ptr [rbp - 0x4c]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    mov ecx, dword ptr [rbp - 0x48]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    mov ecx, dword ptr [rbp - 0x44]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    movapd xmmword ptr [rbp - 0x40], xmm1
; X64-NEXT:    mov ecx, dword ptr [rbp - 0x40]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    mov ecx, dword ptr [rbp - 0x3c]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    mov ecx, dword ptr [rbp - 0x38]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    mov ecx, dword ptr [rbp - 0x34]
; X64-NEXT:    imul eax, ecx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v8i32>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    mov w0, v0.s[0]
; ARM64-NEXT:    mov w1, v0.s[1]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, v0.s[2]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    mov w1, v0.s[3]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, v1.s[0]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    mov w1, v1.s[1]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, v1.s[2]
; ARM64-NEXT:    mul w0, w0, w1
; ARM64-NEXT:    mov w1, v1.s[3]
; ARM64-NEXT:    mul w1, w1, w0
; ARM64-NEXT:    mov w0, w1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <8 x i32>, ptr %p
  %r = call i32 @llvm.vector.reduce.mul(<8 x i32> %v)
  ret i32 %r
}

define i64 @vr_mul_v1i64(ptr %p) {
; X64-LABEL: <vr_mul_v1i64>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    mov rax, qword ptr [rdi]
; X64-NEXT:    mov rcx, rax
; X64-NEXT:    mov rax, rcx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v1i64>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr x1, [x0]
; ARM64-NEXT:    mov x0, x1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <1 x i64>, ptr %p
  %r = call i64 @llvm.vector.reduce.mul(<1 x i64> %v)
  ret i64 %r
}

define i64 @vr_mul_v2i64(ptr %p) {
; X64-LABEL: <vr_mul_v2i64>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop dword ptr [rax]
; X64-NEXT:    movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x40], xmm0
; X64-NEXT:    mov rax, qword ptr [rbp - 0x40]
; X64-NEXT:    mov rcx, qword ptr [rbp - 0x38]
; X64-NEXT:    imul rax, rcx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v2i64>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    mov x0, v0.d[0]
; ARM64-NEXT:    mov x1, v0.d[1]
; ARM64-NEXT:    mul x1, x1, x0
; ARM64-NEXT:    mov x0, x1
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <2 x i64>, ptr %p
  %r = call i64 @llvm.vector.reduce.mul(<2 x i64> %v)
  ret i64 %r
}

define i64 @vr_mul_v5i64(ptr %p) {
; X64-LABEL: <vr_mul_v5i64>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    push rbx
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    nop word ptr [rax + rax]
; X64-NEXT:    mov rax, qword ptr [rdi]
; X64-NEXT:    mov rcx, qword ptr [rdi + 0x8]
; X64-NEXT:    mov rdx, qword ptr [rdi + 0x10]
; X64-NEXT:    mov rbx, qword ptr [rdi + 0x18]
; X64-NEXT:    mov rsi, qword ptr [rdi + 0x20]
; X64-NEXT:    mov rdi, rax
; X64-NEXT:    mov r8, rcx
; X64-NEXT:    imul rdi, r8
; X64-NEXT:    mov r8, rdx
; X64-NEXT:    imul rdi, r8
; X64-NEXT:    mov r8, rbx
; X64-NEXT:    imul rdi, r8
; X64-NEXT:    mov r8, rsi
; X64-NEXT:    imul rdi, r8
; X64-NEXT:    mov rax, rdi
; X64-NEXT:    pop rbx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <vr_mul_v5i64>:
; ARM64:         sub sp, sp, #0xa0
; ARM64-NEXT:    stp x29, x30, [sp]
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    nop
; ARM64-NEXT:    ldr x1, [x0]
; ARM64-NEXT:    ldr x2, [x0, #0x8]
; ARM64-NEXT:    ldr x3, [x0, #0x10]
; ARM64-NEXT:    ldr x4, [x0, #0x18]
; ARM64-NEXT:    ldr x5, [x0, #0x20]
; ARM64-NEXT:    mov x0, x1
; ARM64-NEXT:    mov x6, x2
; ARM64-NEXT:    mul x6, x6, x0
; ARM64-NEXT:    mov x0, x3
; ARM64-NEXT:    mul x0, x0, x6
; ARM64-NEXT:    mov x6, x4
; ARM64-NEXT:    mul x6, x6, x0
; ARM64-NEXT:    mov x0, x5
; ARM64-NEXT:    mul x0, x0, x6
; ARM64-NEXT:    ldp x29, x30, [sp]
; ARM64-NEXT:    add sp, sp, #0xa0
; ARM64-NEXT:    ret
  %v = load <5 x i64>, ptr %p
  %r = call i64 @llvm.vector.reduce.mul(<5 x i64> %v)
  ret i64 %r
}
