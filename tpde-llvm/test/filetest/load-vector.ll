; NOTE: Assertions have been autogenerated by test/update_tpde_llc_test_checks.py UTC_ARGS: --version 5
; SPDX-FileCopyrightText: 2025 Contributors to TPDE <https://tpde.org>
; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

; RUN: tpde-llc --target=x86_64 %s | %objdump | FileCheck %s -check-prefixes=X64
; RUN: tpde-llc --target=aarch64 %s | %objdump | FileCheck %s -check-prefixes=ARM64

define void @load_v1i8(ptr %p) {
; X64-LABEL: <load_v1i8>:
; X64:         movzx eax, byte ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v1i8>:
; ARM64:         ldrb w1, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <1 x i8>, ptr %p
  ret void
}

define void @load_v1i16(ptr %p) {
; X64-LABEL: <load_v1i16>:
; X64:         movzx eax, word ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v1i16>:
; ARM64:         ldrh w1, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <1 x i16>, ptr %p
  ret void
}

define void @load_v1i32(ptr %p) {
; X64-LABEL: <load_v1i32>:
; X64:         mov eax, dword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v1i32>:
; ARM64:         ldr w1, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <1 x i32>, ptr %p
  ret void
}

define void @load_v1i64(ptr %p) {
; X64-LABEL: <load_v1i64>:
; X64:         mov rax, qword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v1i64>:
; ARM64:         ldr x1, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <1 x i64>, ptr %p
  ret void
}

define void @load_v1ptr(ptr %p) {
; X64-LABEL: <load_v1ptr>:
; X64:         mov rax, qword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v1ptr>:
; ARM64:         ldr x1, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <1 x ptr>, ptr %p
  ret void
}

define void @load_v1f32(ptr %p) {
; X64-LABEL: <load_v1f32>:
; X64:         movss xmm0, dword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v1f32>:
; ARM64:         ldr s0, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <1 x float>, ptr %p
  ret void
}

define void @load_v1f64(ptr %p) {
; X64-LABEL: <load_v1f64>:
; X64:         movsd xmm0, qword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v1f64>:
; ARM64:         ldr d0, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <1 x double>, ptr %p
  ret void
}

define void @load_v2i8(ptr %p) {
; X64-LABEL: <load_v2i8>:
; X64:         movzx eax, byte ptr [rdi]
; X64-NEXT:    movzx ecx, byte ptr [rdi + 0x1]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v2i8>:
; ARM64:         ldrb w1, [x0]
; ARM64-NEXT:    ldrb w2, [x0, #0x1]
; ARM64-NEXT:    ret
  %l = load volatile <2 x i8>, ptr %p
  ret void
}

define void @load_v2i16(ptr %p) {
; X64-LABEL: <load_v2i16>:
; X64:         movzx eax, word ptr [rdi]
; X64-NEXT:    movzx ecx, word ptr [rdi + 0x2]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v2i16>:
; ARM64:         ldrh w1, [x0]
; ARM64-NEXT:    ldrh w2, [x0, #0x2]
; ARM64-NEXT:    ret
  %l = load volatile <2 x i16>, ptr %p
  ret void
}

define void @load_v2i32(ptr %p) {
; X64-LABEL: <load_v2i32>:
; X64:         movsd xmm0, qword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v2i32>:
; ARM64:         ldr d0, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <2 x i32>, ptr %p
  ret void
}

define void @load_v2i64(ptr %p) {
; X64-LABEL: <load_v2i64>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v2i64>:
; ARM64:         ldr q0, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <2 x i64>, ptr %p
  ret void
}

define <2 x ptr> @load_v2ptr(ptr %p) {
; X64-LABEL: <load_v2ptr>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v2ptr>:
; ARM64:         ldr q0, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <2 x ptr>, ptr %p
  ret <2 x ptr> %l
}

define void @load_v2f32(ptr %p) {
; X64-LABEL: <load_v2f32>:
; X64:         movsd xmm0, qword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v2f32>:
; ARM64:         ldr d0, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <2 x float>, ptr %p
  ret void
}

define void @load_v2f64(ptr %p) {
; X64-LABEL: <load_v2f64>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v2f64>:
; ARM64:         ldr q0, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <2 x double>, ptr %p
  ret void
}

define void @load_v3i8(ptr %p) {
; X64-LABEL: <load_v3i8>:
; X64:         movzx eax, byte ptr [rdi]
; X64-NEXT:    movzx ecx, byte ptr [rdi + 0x1]
; X64-NEXT:    movzx edx, byte ptr [rdi + 0x2]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v3i8>:
; ARM64:         ldrb w1, [x0]
; ARM64-NEXT:    ldrb w2, [x0, #0x1]
; ARM64-NEXT:    ldrb w3, [x0, #0x2]
; ARM64-NEXT:    ret
  %l = load volatile <3 x i8>, ptr %p
  ret void
}

define void @load_v3i16(ptr %p) {
; X64-LABEL: <load_v3i16>:
; X64:         movzx eax, word ptr [rdi]
; X64-NEXT:    movzx ecx, word ptr [rdi + 0x2]
; X64-NEXT:    movzx edx, word ptr [rdi + 0x4]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v3i16>:
; ARM64:         ldrh w1, [x0]
; ARM64-NEXT:    ldrh w2, [x0, #0x2]
; ARM64-NEXT:    ldrh w3, [x0, #0x4]
; ARM64-NEXT:    ret
  %l = load volatile <3 x i16>, ptr %p
  ret void
}

define void @load_v3i32(ptr %p) {
; X64-LABEL: <load_v3i32>:
; X64:         mov eax, dword ptr [rdi]
; X64-NEXT:    mov ecx, dword ptr [rdi + 0x4]
; X64-NEXT:    mov edx, dword ptr [rdi + 0x8]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v3i32>:
; ARM64:         ldr w1, [x0]
; ARM64-NEXT:    ldr w2, [x0, #0x4]
; ARM64-NEXT:    ldr w3, [x0, #0x8]
; ARM64-NEXT:    ret
  %l = load volatile <3 x i32>, ptr %p
  ret void
}

define void @load_v3i64(ptr %p) {
; X64-LABEL: <load_v3i64>:
; X64:         mov rax, qword ptr [rdi]
; X64-NEXT:    mov rcx, qword ptr [rdi + 0x8]
; X64-NEXT:    mov rdx, qword ptr [rdi + 0x10]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v3i64>:
; ARM64:         ldr x1, [x0]
; ARM64-NEXT:    ldr x2, [x0, #0x8]
; ARM64-NEXT:    ldr x3, [x0, #0x10]
; ARM64-NEXT:    ret
  %l = load volatile <3 x i64>, ptr %p
  ret void
}

define void @load_v3ptr(ptr %p) {
; X64-LABEL: <load_v3ptr>:
; X64:         mov rax, qword ptr [rdi]
; X64-NEXT:    mov rcx, qword ptr [rdi + 0x8]
; X64-NEXT:    mov rdx, qword ptr [rdi + 0x10]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v3ptr>:
; ARM64:         ldr x1, [x0]
; ARM64-NEXT:    ldr x2, [x0, #0x8]
; ARM64-NEXT:    ldr x3, [x0, #0x10]
; ARM64-NEXT:    ret
  %l = load volatile <3 x ptr>, ptr %p
  ret void
}

define void @load_v3f32(ptr %p) {
; X64-LABEL: <load_v3f32>:
; X64:         movss xmm0, dword ptr [rdi]
; X64-NEXT:    movss xmm1, dword ptr [rdi + 0x4]
; X64-NEXT:    movss xmm2, dword ptr [rdi + 0x8]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v3f32>:
; ARM64:         ldr s0, [x0]
; ARM64-NEXT:    ldr s1, [x0, #0x4]
; ARM64-NEXT:    ldr s2, [x0, #0x8]
; ARM64-NEXT:    ret
  %l = load volatile <3 x float>, ptr %p
  ret void
}

define void @load_v3f64(ptr %p) {
; X64-LABEL: <load_v3f64>:
; X64:         movsd xmm0, qword ptr [rdi]
; X64-NEXT:    movsd xmm1, qword ptr [rdi + 0x8]
; X64-NEXT:    movsd xmm2, qword ptr [rdi + 0x10]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v3f64>:
; ARM64:         ldr d0, [x0]
; ARM64-NEXT:    ldr d1, [x0, #0x8]
; ARM64-NEXT:    ldr d2, [x0, #0x10]
; ARM64-NEXT:    ret
  %l = load volatile <3 x double>, ptr %p
  ret void
}

define void @load_v4i8(ptr %p) {
; X64-LABEL: <load_v4i8>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    push rbx
; X64-NEXT:    movzx eax, byte ptr [rdi]
; X64-NEXT:    movzx ecx, byte ptr [rdi + 0x1]
; X64-NEXT:    movzx edx, byte ptr [rdi + 0x2]
; X64-NEXT:    movzx ebx, byte ptr [rdi + 0x3]
; X64-NEXT:    pop rbx
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v4i8>:
; ARM64:         ldrb w1, [x0]
; ARM64-NEXT:    ldrb w2, [x0, #0x1]
; ARM64-NEXT:    ldrb w3, [x0, #0x2]
; ARM64-NEXT:    ldrb w4, [x0, #0x3]
; ARM64-NEXT:    ret
  %l = load volatile <4 x i8>, ptr %p
  ret void
}

define void @load_v4i16(ptr %p) {
; X64-LABEL: <load_v4i16>:
; X64:         movsd xmm0, qword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v4i16>:
; ARM64:         ldr d0, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <4 x i16>, ptr %p
  ret void
}

define void @load_v4i32(ptr %p) {
; X64-LABEL: <load_v4i32>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v4i32>:
; ARM64:         ldr q0, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <4 x i32>, ptr %p
  ret void
}

define void @load_v4i64(ptr %p) {
; X64-LABEL: <load_v4i64>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v4i64>:
; ARM64:         ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    ret
  %l = load volatile <4 x i64>, ptr %p
  ret void
}

define void @load_v4ptr(ptr %p) {
; X64-LABEL: <load_v4ptr>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v4ptr>:
; ARM64:         ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    ret
  %l = load volatile <4 x ptr>, ptr %p
  ret void
}

define void @load_v4f32(ptr %p) {
; X64-LABEL: <load_v4f32>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v4f32>:
; ARM64:         ldr q0, [x0]
; ARM64-NEXT:    ret
  %l = load volatile <4 x float>, ptr %p
  ret void
}

define void @load_v4f64(ptr %p) {
; X64-LABEL: <load_v4f64>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v4f64>:
; ARM64:         ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    ret
  %l = load volatile <4 x double>, ptr %p
  ret void
}

define void @load_v64i8(ptr %p) {
; X64-LABEL: <load_v64i8>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    movups xmm2, xmmword ptr [rdi + 0x20]
; X64-NEXT:    movups xmm3, xmmword ptr [rdi + 0x30]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v64i8>:
; ARM64:         ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    ldr q2, [x0, #0x20]
; ARM64-NEXT:    ldr q3, [x0, #0x30]
; ARM64-NEXT:    ret
  %l = load volatile <64 x i8>, ptr %p
  ret void
}

define void @load_v64i16(ptr %p) {
; X64-LABEL: <load_v64i16>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    movups xmm2, xmmword ptr [rdi + 0x20]
; X64-NEXT:    movups xmm3, xmmword ptr [rdi + 0x30]
; X64-NEXT:    movups xmm4, xmmword ptr [rdi + 0x40]
; X64-NEXT:    movups xmm5, xmmword ptr [rdi + 0x50]
; X64-NEXT:    movups xmm6, xmmword ptr [rdi + 0x60]
; X64-NEXT:    movups xmm7, xmmword ptr [rdi + 0x70]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v64i16>:
; ARM64:         ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    ldr q2, [x0, #0x20]
; ARM64-NEXT:    ldr q3, [x0, #0x30]
; ARM64-NEXT:    ldr q4, [x0, #0x40]
; ARM64-NEXT:    ldr q5, [x0, #0x50]
; ARM64-NEXT:    ldr q6, [x0, #0x60]
; ARM64-NEXT:    ldr q7, [x0, #0x70]
; ARM64-NEXT:    ret
  %l = load volatile <64 x i16>, ptr %p
  ret void
}

define void @load_v64i32(ptr %p) {
; X64-LABEL: <load_v64i32>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    movups xmm2, xmmword ptr [rdi + 0x20]
; X64-NEXT:    movups xmm3, xmmword ptr [rdi + 0x30]
; X64-NEXT:    movups xmm4, xmmword ptr [rdi + 0x40]
; X64-NEXT:    movups xmm5, xmmword ptr [rdi + 0x50]
; X64-NEXT:    movups xmm6, xmmword ptr [rdi + 0x60]
; X64-NEXT:    movups xmm7, xmmword ptr [rdi + 0x70]
; X64-NEXT:    movups xmm8, xmmword ptr [rdi + 0x80]
; X64-NEXT:    movups xmm9, xmmword ptr [rdi + 0x90]
; X64-NEXT:    movups xmm10, xmmword ptr [rdi + 0xa0]
; X64-NEXT:    movups xmm11, xmmword ptr [rdi + 0xb0]
; X64-NEXT:    movups xmm12, xmmword ptr [rdi + 0xc0]
; X64-NEXT:    movups xmm13, xmmword ptr [rdi + 0xd0]
; X64-NEXT:    movups xmm14, xmmword ptr [rdi + 0xe0]
; X64-NEXT:    movups xmm15, xmmword ptr [rdi + 0xf0]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v64i32>:
; ARM64:         stp x29, x30, [sp, #-0xa0]!
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    stp d8, d9, [sp, #0x10]
; ARM64-NEXT:    stp d10, d11, [sp, #0x20]
; ARM64-NEXT:    stp d12, d13, [sp, #0x30]
; ARM64-NEXT:    stp d14, d15, [sp, #0x40]
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    ldr q2, [x0, #0x20]
; ARM64-NEXT:    ldr q3, [x0, #0x30]
; ARM64-NEXT:    ldr q4, [x0, #0x40]
; ARM64-NEXT:    ldr q5, [x0, #0x50]
; ARM64-NEXT:    ldr q6, [x0, #0x60]
; ARM64-NEXT:    ldr q7, [x0, #0x70]
; ARM64-NEXT:    ldr q8, [x0, #0x80]
; ARM64-NEXT:    ldr q9, [x0, #0x90]
; ARM64-NEXT:    ldr q10, [x0, #0xa0]
; ARM64-NEXT:    ldr q11, [x0, #0xb0]
; ARM64-NEXT:    ldr q12, [x0, #0xc0]
; ARM64-NEXT:    ldr q13, [x0, #0xd0]
; ARM64-NEXT:    ldr q14, [x0, #0xe0]
; ARM64-NEXT:    ldr q15, [x0, #0xf0]
; ARM64-NEXT:    ldp d8, d9, [sp, #0x10]
; ARM64-NEXT:    ldp d10, d11, [sp, #0x20]
; ARM64-NEXT:    ldp d12, d13, [sp, #0x30]
; ARM64-NEXT:    ldp d14, d15, [sp, #0x40]
; ARM64-NEXT:    ldp x29, x30, [sp], #0xa0
; ARM64-NEXT:    ret
  %l = load volatile <64 x i32>, ptr %p
  ret void
}

define void @load_v64i64(ptr %p) {
; X64-LABEL: <load_v64i64>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    sub rsp, 0x230
; X64-NEXT:    movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    movups xmm2, xmmword ptr [rdi + 0x20]
; X64-NEXT:    movups xmm3, xmmword ptr [rdi + 0x30]
; X64-NEXT:    movups xmm4, xmmword ptr [rdi + 0x40]
; X64-NEXT:    movups xmm5, xmmword ptr [rdi + 0x50]
; X64-NEXT:    movups xmm6, xmmword ptr [rdi + 0x60]
; X64-NEXT:    movups xmm7, xmmword ptr [rdi + 0x70]
; X64-NEXT:    movups xmm8, xmmword ptr [rdi + 0x80]
; X64-NEXT:    movups xmm9, xmmword ptr [rdi + 0x90]
; X64-NEXT:    movups xmm10, xmmword ptr [rdi + 0xa0]
; X64-NEXT:    movups xmm11, xmmword ptr [rdi + 0xb0]
; X64-NEXT:    movups xmm12, xmmword ptr [rdi + 0xc0]
; X64-NEXT:    movups xmm13, xmmword ptr [rdi + 0xd0]
; X64-NEXT:    movups xmm14, xmmword ptr [rdi + 0xe0]
; X64-NEXT:    movups xmm15, xmmword ptr [rdi + 0xf0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x230], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x100]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x130], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x110]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x120], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x120]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x110], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x130]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x100], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x140]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xf0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x150]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xe0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x160]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xd0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x170]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xc0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x180]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xb0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x190]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xa0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1a0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x90], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1b0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x80], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1c0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x70], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1d0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x60], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1e0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x50], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1f0]
; X64-NEXT:    add rsp, 0x230
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v64i64>:
; ARM64:         stp x29, x30, [sp, #-0xa0]!
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    stp d8, d9, [sp, #0x10]
; ARM64-NEXT:    stp d10, d11, [sp, #0x20]
; ARM64-NEXT:    stp d12, d13, [sp, #0x30]
; ARM64-NEXT:    stp d14, d15, [sp, #0x40]
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    ldr q2, [x0, #0x20]
; ARM64-NEXT:    ldr q3, [x0, #0x30]
; ARM64-NEXT:    ldr q4, [x0, #0x40]
; ARM64-NEXT:    ldr q5, [x0, #0x50]
; ARM64-NEXT:    ldr q6, [x0, #0x60]
; ARM64-NEXT:    ldr q7, [x0, #0x70]
; ARM64-NEXT:    ldr q8, [x0, #0x80]
; ARM64-NEXT:    ldr q9, [x0, #0x90]
; ARM64-NEXT:    ldr q10, [x0, #0xa0]
; ARM64-NEXT:    ldr q11, [x0, #0xb0]
; ARM64-NEXT:    ldr q12, [x0, #0xc0]
; ARM64-NEXT:    ldr q13, [x0, #0xd0]
; ARM64-NEXT:    ldr q14, [x0, #0xe0]
; ARM64-NEXT:    ldr q15, [x0, #0xf0]
; ARM64-NEXT:    ldr q16, [x0, #0x100]
; ARM64-NEXT:    ldr q17, [x0, #0x110]
; ARM64-NEXT:    ldr q18, [x0, #0x120]
; ARM64-NEXT:    ldr q19, [x0, #0x130]
; ARM64-NEXT:    ldr q20, [x0, #0x140]
; ARM64-NEXT:    ldr q21, [x0, #0x150]
; ARM64-NEXT:    ldr q22, [x0, #0x160]
; ARM64-NEXT:    ldr q23, [x0, #0x170]
; ARM64-NEXT:    ldr q24, [x0, #0x180]
; ARM64-NEXT:    ldr q25, [x0, #0x190]
; ARM64-NEXT:    ldr q26, [x0, #0x1a0]
; ARM64-NEXT:    ldr q27, [x0, #0x1b0]
; ARM64-NEXT:    ldr q28, [x0, #0x1c0]
; ARM64-NEXT:    ldr q29, [x0, #0x1d0]
; ARM64-NEXT:    ldr q30, [x0, #0x1e0]
; ARM64-NEXT:    ldr q31, [x0, #0x1f0]
; ARM64-NEXT:    ldp d8, d9, [sp, #0x10]
; ARM64-NEXT:    ldp d10, d11, [sp, #0x20]
; ARM64-NEXT:    ldp d12, d13, [sp, #0x30]
; ARM64-NEXT:    ldp d14, d15, [sp, #0x40]
; ARM64-NEXT:    ldp x29, x30, [sp], #0xa0
; ARM64-NEXT:    ret
  %l = load volatile <64 x i64>, ptr %p
  ret void
}

define void @load_v64ptr(ptr %p) {
; X64-LABEL: <load_v64ptr>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    sub rsp, 0x230
; X64-NEXT:    movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    movups xmm2, xmmword ptr [rdi + 0x20]
; X64-NEXT:    movups xmm3, xmmword ptr [rdi + 0x30]
; X64-NEXT:    movups xmm4, xmmword ptr [rdi + 0x40]
; X64-NEXT:    movups xmm5, xmmword ptr [rdi + 0x50]
; X64-NEXT:    movups xmm6, xmmword ptr [rdi + 0x60]
; X64-NEXT:    movups xmm7, xmmword ptr [rdi + 0x70]
; X64-NEXT:    movups xmm8, xmmword ptr [rdi + 0x80]
; X64-NEXT:    movups xmm9, xmmword ptr [rdi + 0x90]
; X64-NEXT:    movups xmm10, xmmword ptr [rdi + 0xa0]
; X64-NEXT:    movups xmm11, xmmword ptr [rdi + 0xb0]
; X64-NEXT:    movups xmm12, xmmword ptr [rdi + 0xc0]
; X64-NEXT:    movups xmm13, xmmword ptr [rdi + 0xd0]
; X64-NEXT:    movups xmm14, xmmword ptr [rdi + 0xe0]
; X64-NEXT:    movups xmm15, xmmword ptr [rdi + 0xf0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x230], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x100]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x130], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x110]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x120], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x120]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x110], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x130]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x100], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x140]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xf0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x150]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xe0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x160]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xd0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x170]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xc0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x180]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xb0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x190]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xa0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1a0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x90], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1b0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x80], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1c0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x70], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1d0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x60], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1e0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x50], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1f0]
; X64-NEXT:    add rsp, 0x230
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v64ptr>:
; ARM64:         stp x29, x30, [sp, #-0xa0]!
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    stp d8, d9, [sp, #0x10]
; ARM64-NEXT:    stp d10, d11, [sp, #0x20]
; ARM64-NEXT:    stp d12, d13, [sp, #0x30]
; ARM64-NEXT:    stp d14, d15, [sp, #0x40]
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    ldr q2, [x0, #0x20]
; ARM64-NEXT:    ldr q3, [x0, #0x30]
; ARM64-NEXT:    ldr q4, [x0, #0x40]
; ARM64-NEXT:    ldr q5, [x0, #0x50]
; ARM64-NEXT:    ldr q6, [x0, #0x60]
; ARM64-NEXT:    ldr q7, [x0, #0x70]
; ARM64-NEXT:    ldr q8, [x0, #0x80]
; ARM64-NEXT:    ldr q9, [x0, #0x90]
; ARM64-NEXT:    ldr q10, [x0, #0xa0]
; ARM64-NEXT:    ldr q11, [x0, #0xb0]
; ARM64-NEXT:    ldr q12, [x0, #0xc0]
; ARM64-NEXT:    ldr q13, [x0, #0xd0]
; ARM64-NEXT:    ldr q14, [x0, #0xe0]
; ARM64-NEXT:    ldr q15, [x0, #0xf0]
; ARM64-NEXT:    ldr q16, [x0, #0x100]
; ARM64-NEXT:    ldr q17, [x0, #0x110]
; ARM64-NEXT:    ldr q18, [x0, #0x120]
; ARM64-NEXT:    ldr q19, [x0, #0x130]
; ARM64-NEXT:    ldr q20, [x0, #0x140]
; ARM64-NEXT:    ldr q21, [x0, #0x150]
; ARM64-NEXT:    ldr q22, [x0, #0x160]
; ARM64-NEXT:    ldr q23, [x0, #0x170]
; ARM64-NEXT:    ldr q24, [x0, #0x180]
; ARM64-NEXT:    ldr q25, [x0, #0x190]
; ARM64-NEXT:    ldr q26, [x0, #0x1a0]
; ARM64-NEXT:    ldr q27, [x0, #0x1b0]
; ARM64-NEXT:    ldr q28, [x0, #0x1c0]
; ARM64-NEXT:    ldr q29, [x0, #0x1d0]
; ARM64-NEXT:    ldr q30, [x0, #0x1e0]
; ARM64-NEXT:    ldr q31, [x0, #0x1f0]
; ARM64-NEXT:    ldp d8, d9, [sp, #0x10]
; ARM64-NEXT:    ldp d10, d11, [sp, #0x20]
; ARM64-NEXT:    ldp d12, d13, [sp, #0x30]
; ARM64-NEXT:    ldp d14, d15, [sp, #0x40]
; ARM64-NEXT:    ldp x29, x30, [sp], #0xa0
; ARM64-NEXT:    ret
  %l = load volatile <64 x ptr>, ptr %p
  ret void
}

define void @load_v64f32(ptr %p) {
; X64-LABEL: <load_v64f32>:
; X64:         movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    movups xmm2, xmmword ptr [rdi + 0x20]
; X64-NEXT:    movups xmm3, xmmword ptr [rdi + 0x30]
; X64-NEXT:    movups xmm4, xmmword ptr [rdi + 0x40]
; X64-NEXT:    movups xmm5, xmmword ptr [rdi + 0x50]
; X64-NEXT:    movups xmm6, xmmword ptr [rdi + 0x60]
; X64-NEXT:    movups xmm7, xmmword ptr [rdi + 0x70]
; X64-NEXT:    movups xmm8, xmmword ptr [rdi + 0x80]
; X64-NEXT:    movups xmm9, xmmword ptr [rdi + 0x90]
; X64-NEXT:    movups xmm10, xmmword ptr [rdi + 0xa0]
; X64-NEXT:    movups xmm11, xmmword ptr [rdi + 0xb0]
; X64-NEXT:    movups xmm12, xmmword ptr [rdi + 0xc0]
; X64-NEXT:    movups xmm13, xmmword ptr [rdi + 0xd0]
; X64-NEXT:    movups xmm14, xmmword ptr [rdi + 0xe0]
; X64-NEXT:    movups xmm15, xmmword ptr [rdi + 0xf0]
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v64f32>:
; ARM64:         stp x29, x30, [sp, #-0xa0]!
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    stp d8, d9, [sp, #0x10]
; ARM64-NEXT:    stp d10, d11, [sp, #0x20]
; ARM64-NEXT:    stp d12, d13, [sp, #0x30]
; ARM64-NEXT:    stp d14, d15, [sp, #0x40]
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    ldr q2, [x0, #0x20]
; ARM64-NEXT:    ldr q3, [x0, #0x30]
; ARM64-NEXT:    ldr q4, [x0, #0x40]
; ARM64-NEXT:    ldr q5, [x0, #0x50]
; ARM64-NEXT:    ldr q6, [x0, #0x60]
; ARM64-NEXT:    ldr q7, [x0, #0x70]
; ARM64-NEXT:    ldr q8, [x0, #0x80]
; ARM64-NEXT:    ldr q9, [x0, #0x90]
; ARM64-NEXT:    ldr q10, [x0, #0xa0]
; ARM64-NEXT:    ldr q11, [x0, #0xb0]
; ARM64-NEXT:    ldr q12, [x0, #0xc0]
; ARM64-NEXT:    ldr q13, [x0, #0xd0]
; ARM64-NEXT:    ldr q14, [x0, #0xe0]
; ARM64-NEXT:    ldr q15, [x0, #0xf0]
; ARM64-NEXT:    ldp d8, d9, [sp, #0x10]
; ARM64-NEXT:    ldp d10, d11, [sp, #0x20]
; ARM64-NEXT:    ldp d12, d13, [sp, #0x30]
; ARM64-NEXT:    ldp d14, d15, [sp, #0x40]
; ARM64-NEXT:    ldp x29, x30, [sp], #0xa0
; ARM64-NEXT:    ret
  %l = load volatile <64 x float>, ptr %p
  ret void
}

define void @load_v64f64(ptr %p) {
; X64-LABEL: <load_v64f64>:
; X64:         push rbp
; X64-NEXT:    mov rbp, rsp
; X64-NEXT:    sub rsp, 0x230
; X64-NEXT:    movups xmm0, xmmword ptr [rdi]
; X64-NEXT:    movups xmm1, xmmword ptr [rdi + 0x10]
; X64-NEXT:    movups xmm2, xmmword ptr [rdi + 0x20]
; X64-NEXT:    movups xmm3, xmmword ptr [rdi + 0x30]
; X64-NEXT:    movups xmm4, xmmword ptr [rdi + 0x40]
; X64-NEXT:    movups xmm5, xmmword ptr [rdi + 0x50]
; X64-NEXT:    movups xmm6, xmmword ptr [rdi + 0x60]
; X64-NEXT:    movups xmm7, xmmword ptr [rdi + 0x70]
; X64-NEXT:    movups xmm8, xmmword ptr [rdi + 0x80]
; X64-NEXT:    movups xmm9, xmmword ptr [rdi + 0x90]
; X64-NEXT:    movups xmm10, xmmword ptr [rdi + 0xa0]
; X64-NEXT:    movups xmm11, xmmword ptr [rdi + 0xb0]
; X64-NEXT:    movups xmm12, xmmword ptr [rdi + 0xc0]
; X64-NEXT:    movups xmm13, xmmword ptr [rdi + 0xd0]
; X64-NEXT:    movups xmm14, xmmword ptr [rdi + 0xe0]
; X64-NEXT:    movups xmm15, xmmword ptr [rdi + 0xf0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x230], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x100]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x130], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x110]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x120], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x120]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x110], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x130]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x100], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x140]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xf0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x150]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xe0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x160]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xd0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x170]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xc0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x180]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xb0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x190]
; X64-NEXT:    movapd xmmword ptr [rbp - 0xa0], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1a0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x90], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1b0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x80], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1c0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x70], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1d0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x60], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1e0]
; X64-NEXT:    movapd xmmword ptr [rbp - 0x50], xmm0
; X64-NEXT:    movups xmm0, xmmword ptr [rdi + 0x1f0]
; X64-NEXT:    add rsp, 0x230
; X64-NEXT:    pop rbp
; X64-NEXT:    ret
;
; ARM64-LABEL: <load_v64f64>:
; ARM64:         stp x29, x30, [sp, #-0xa0]!
; ARM64-NEXT:    mov x29, sp
; ARM64-NEXT:    stp d8, d9, [sp, #0x10]
; ARM64-NEXT:    stp d10, d11, [sp, #0x20]
; ARM64-NEXT:    stp d12, d13, [sp, #0x30]
; ARM64-NEXT:    stp d14, d15, [sp, #0x40]
; ARM64-NEXT:    ldr q0, [x0]
; ARM64-NEXT:    ldr q1, [x0, #0x10]
; ARM64-NEXT:    ldr q2, [x0, #0x20]
; ARM64-NEXT:    ldr q3, [x0, #0x30]
; ARM64-NEXT:    ldr q4, [x0, #0x40]
; ARM64-NEXT:    ldr q5, [x0, #0x50]
; ARM64-NEXT:    ldr q6, [x0, #0x60]
; ARM64-NEXT:    ldr q7, [x0, #0x70]
; ARM64-NEXT:    ldr q8, [x0, #0x80]
; ARM64-NEXT:    ldr q9, [x0, #0x90]
; ARM64-NEXT:    ldr q10, [x0, #0xa0]
; ARM64-NEXT:    ldr q11, [x0, #0xb0]
; ARM64-NEXT:    ldr q12, [x0, #0xc0]
; ARM64-NEXT:    ldr q13, [x0, #0xd0]
; ARM64-NEXT:    ldr q14, [x0, #0xe0]
; ARM64-NEXT:    ldr q15, [x0, #0xf0]
; ARM64-NEXT:    ldr q16, [x0, #0x100]
; ARM64-NEXT:    ldr q17, [x0, #0x110]
; ARM64-NEXT:    ldr q18, [x0, #0x120]
; ARM64-NEXT:    ldr q19, [x0, #0x130]
; ARM64-NEXT:    ldr q20, [x0, #0x140]
; ARM64-NEXT:    ldr q21, [x0, #0x150]
; ARM64-NEXT:    ldr q22, [x0, #0x160]
; ARM64-NEXT:    ldr q23, [x0, #0x170]
; ARM64-NEXT:    ldr q24, [x0, #0x180]
; ARM64-NEXT:    ldr q25, [x0, #0x190]
; ARM64-NEXT:    ldr q26, [x0, #0x1a0]
; ARM64-NEXT:    ldr q27, [x0, #0x1b0]
; ARM64-NEXT:    ldr q28, [x0, #0x1c0]
; ARM64-NEXT:    ldr q29, [x0, #0x1d0]
; ARM64-NEXT:    ldr q30, [x0, #0x1e0]
; ARM64-NEXT:    ldr q31, [x0, #0x1f0]
; ARM64-NEXT:    ldp d8, d9, [sp, #0x10]
; ARM64-NEXT:    ldp d10, d11, [sp, #0x20]
; ARM64-NEXT:    ldp d12, d13, [sp, #0x30]
; ARM64-NEXT:    ldp d14, d15, [sp, #0x40]
; ARM64-NEXT:    ldp x29, x30, [sp], #0xa0
; ARM64-NEXT:    ret
  %l = load volatile <64 x double>, ptr %p
  ret void
}
